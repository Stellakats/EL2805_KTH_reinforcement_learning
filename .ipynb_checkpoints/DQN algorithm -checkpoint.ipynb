{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB2: Question c\n",
    "\n",
    "*Write pseudo-code for DQN and explain it line by line. Associate your pseudo-code line\n",
    "numbers with corresponding (and potentially missing) parts of the code provided to you.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this pseudocode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The rest of this notebook mentions the lines of code that correspond to the pseudocode depicted above. \n",
    "\n",
    "Thus, the code snippets mentioned below cannot be used in the exact order mentioned. They are put in this order just to demonstrate the lines of code that correspond to the aforementioned pseudocode, as part of the answer of question (c) of Lab2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization of θ and φ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.discount_factor = 0.95\n",
    "self.learning_rate = 0.005\n",
    "self.epsilon = 0.02 #Fixed\n",
    "self.batch_size = 32 #Fixed\n",
    "self.memory_size = 1000\n",
    "self.train_start = 1000 #Fixed\n",
    "self.target_update_frequency = 1\n",
    "self.model = self.build_model()\n",
    "self.target_model = self.build_model()\n",
    "self.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization of replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create memory buffer using deque\n",
    "self.memory = deque(maxlen=self.memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of state 1: we find this line of code in the main. We also do the essential reshaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterations** (Specifically, line 1 iterates through episdes and line 11 iterates over time steps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "        score = 0\n",
    "        state = env.reset() #Initialize/reset the environment\n",
    "        state = np.reshape(state, [1, state_size]) #Reshape state so that to a 1 by state_size two-dimensional array ie. [x_1,x_2] to [[x_1,x_2]]\n",
    "        #Compute Q values for plotting\n",
    "        tmp = agent.model.predict(test_states)\n",
    "        max_q[e][:] = np.max(tmp, axis=1)\n",
    "        max_q_mean[e] = np.mean(max_q[e][:])\n",
    "\n",
    "        while not done:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute π_t, ε-greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = agent.get_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborating on this step, the \".get_action\" function that returns the next action according to the epsilon greedy policy is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "    random_int = np.random.binomial(1, (1 - self.epsilon))\n",
    "    if random_int == 1:\n",
    "        qs = self.model.predict(state) #model is going to give me two q values, one for each action that can be taken from state.\n",
    "        action = np.argmax(qs)\n",
    "    elif random_int == 0:\n",
    "        action = random.randrange(self.action_size)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then execute the selected according to π_t action in the emulator (env.step) and observe r_t and s_(t+1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store s_t, a_t, r_t, s_(t+1) in the replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.append_sample(state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample k experiences from replay memory: This is part of the \"agent.train_model\" function but the sampling explicitly takes place in the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in [1,k]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(self.batch_size):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute using the target net with weights φ:\n",
    "\n",
    "\n",
    "if episode stops at s_(t+1):\n",
    "    y_i = r_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if done[i]:\n",
    "    target[i][action[i]] = reward[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    target[i][action[i]] = reward[i] + self.discount_factor * (np.max(target_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update the weights, θ, using backrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every c steps update the target network's  φ, with θ parametres learnt so far.\n",
    "\n",
    "The corresponding line can be found in the main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if e % agent.target_update_frequency == 0:\n",
    "    agent.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the update frequency is 1, then the update will be conducted after each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB2: Question d\n",
    "\n",
    "*Consult the Keras documentation and briefly explain the layout of the given neural network model. Note that we use a network which takes only the state as input and outputs the Q values for each action at that state.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet that contains the network is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential model used here is a linear stack of layers.\n",
    "\n",
    "\n",
    "We use the .add method to add layers. In detail:\n",
    "\n",
    "**1st layer:** First we add a regular densely-connected NN layer of 16 units. This kind of layers can support the specification of their input shape via the argument input_dim, so we specify it to be equal to the number of states which is 4. We specify the desired activation of this first layer via the argument activation='relu' and initialize the weights using kernel_initializer='he_uniform. This initializer draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd layer:** this is going to be the second and last layer of this simple network, consisting of 2 units (as many as the actions we can take out of the state that was fed into the network, that is equal to self.action_size), using a linear activation and the same initializer as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
